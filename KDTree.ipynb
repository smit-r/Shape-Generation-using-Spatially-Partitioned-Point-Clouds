{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDTree.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smit-r/Shape-Generation-using-Spatially-Partitioned-Point-Clouds/blob/main/KDTree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xujXfqvMaR7i"
      },
      "source": [
        "!python -m pip install open3d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr6NOZFVDMM7"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as scy\n",
        "import os\n",
        "import open3d\n",
        "import numpy as np\n",
        "import sklearn.decomposition"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXfUcQlsaEwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10805e4c-34d9-45fe-92d8-16e0dbc20db4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvMiK8q-aFrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513780d1-9ed6-4449-9319-56a57325366d"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/shapenet_chair_data/shapenet-chairs-pcd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/shapenet_chair_data/shapenet-chairs-pcd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yu-z1DPaVr_"
      },
      "source": [
        "#### KDTree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMtK_pKRDv_B"
      },
      "source": [
        "## performs kdtree sorting on pointcloud\n",
        "def kdtree(pointCloud, level):\n",
        "  if (len(pointCloud)<=1):\n",
        "    return pointCloud\n",
        "  pointlist = np.array(pointCloud)\n",
        "  plane = getPlane(level)\n",
        "  median = getkthPosition(pointlist, plane, 0, len(pointlist)-1, int(len(pointlist)/2))\n",
        "  leftArray = median[:int(len(median)/2)]\n",
        "  rightArray = median[int((len(median)/2))+1:]\n",
        "  leftSortedArray = kdtree(leftArray, level+1)\n",
        "  rightSortedArray = kdtree(rightArray, level+1)\n",
        "  return np.concatenate((leftSortedArray, [median[int(len(median)/2)]], rightSortedArray))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njkozMnBEL3a"
      },
      "source": [
        "def getPlane(level):\n",
        "  plane = level%3\n",
        "  return plane"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnKgdpluHLEm"
      },
      "source": [
        "def getPartition(pointlist, axis, l, r):\n",
        "  i = l\n",
        "  x = pointlist[r, axis]\n",
        "  for j in range(l, r):\n",
        "    if pointlist[j,axis]<x:\n",
        "      pointlist[[i,j]] = pointlist[[j,i]]\n",
        "      i += 1\n",
        "  pointlist[[i,r]] = pointlist[[r,i]]\n",
        "  return i"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txBstj68WoLr"
      },
      "source": [
        "def getkthPosition(pointlist, axis, l, r, k):\n",
        "  i = getPartition(pointlist, axis, l, r)\n",
        "  if (l>r):\n",
        "    return np.array([])\n",
        "  if (i == k):\n",
        "    return pointlist\n",
        "  elif (i == r):\n",
        "    return getkthPosition(pointlist, axis, l, r-1, k)\n",
        "  elif (i < k):\n",
        "    return getkthPosition(pointlist, axis, i+1, r, k)\n",
        "  else:\n",
        "    return getkthPosition(pointlist, axis, l, i-1, k)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKFhWBGwVPRM"
      },
      "source": [
        "#### PCA Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6-U06A8WUTh"
      },
      "source": [
        "## load the dataset\n",
        "dataset = []\n",
        "for f in os.listdir():\n",
        "  if f[-4:]==\".pcd\":\n",
        "    pcd = open3d.io.read_point_cloud(f)\n",
        "    pointcloud = np.asarray(pcd.points)\n",
        "    dataset.append(pointcloud)\n",
        "  print(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqEG_AiMWzeg"
      },
      "source": [
        "# sort using KDTree\n",
        "data = np.array(dataset)\n",
        "for i in range(len(dataset)):\n",
        "  shape = data[i]\n",
        "  shape = kdtree(shape, 0)\n",
        "  data[i] = shape\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PziYB7X1W-S4"
      },
      "source": [
        "data = np.load('/content/drive/My Drive/shapenetdata.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifBiICfqXsh2",
        "outputId": "e896a9bb-ebbd-494b-ce69-e56e7e5ffca7"
      },
      "source": [
        "# construct matrix P\n",
        "P = np.reshape(data, [5000,-1]) #(5000, 3000)\n",
        "mean = np.mean(P, axis=0) #(5000, )\n",
        "Pu = P - mean #P[i]-mean\n",
        "Pu.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 3000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyx828f1YlG0"
      },
      "source": [
        "# PCA\n",
        "def pcaAnalysis(Pu):\n",
        "  pca = sklearn.decomposition.PCA(n_components=100)\n",
        "  pca.fit(Pu)\n",
        "  comp = pca.components_\n",
        "  return comp"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP_Go7fIH3R6"
      },
      "source": [
        "comp = pcaAnalysis(Pu)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQvIQ_VHZagd",
        "outputId": "155a2034-c718-4f64-d2ab-53c5989646bc"
      },
      "source": [
        "# transform and reconstruct\n",
        "Pcomp = np.dot(Pu, comp.T)\n",
        "print(Pcomp.shape)\n",
        "Precon = np.dot(Pcomp, comp)\n",
        "print(Precon.shape)\n",
        "errors = Pu - Precon\n",
        "errors = errors*errors #calculate errors for each shape\n",
        "print(errors.shape)\n",
        "errors = np.sum(errors, axis=1) #(5000, )\n",
        "print(errors.shape)\n",
        "errlist = [errors]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 100)\n",
            "(5000, 3000)\n",
            "(5000, 3000)\n",
            "(5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqINs-3_ag9Y"
      },
      "source": [
        "# sample two points randomly\n",
        "def samplePoints():\n",
        "  points = np.random.randint(0,1000, 2)\n",
        "  return points"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSLr--ScIVD"
      },
      "source": [
        "# swap points\n",
        "def swap(shape, points):\n",
        "  shape[3*points[0]], shape[3*points[1]] = shape[3*points[1]], shape[3*points[0]] # swap x\n",
        "  shape[3*points[0]+1], shape[3*points[1]+1] = shape[3*points[1]+1], shape[3*points[0]+1] #swap y\n",
        "  shape[3*points[0]+2], shape[3*points[1]+2] = shape[3*points[1]+2], shape[3*points[0]+2] #swap z\n",
        "  return shape"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAaLBHqtg5mr"
      },
      "source": [
        "# calculate error\n",
        "def error(shape, shape_):\n",
        "  err = shape - shape_\n",
        "  err = np.dot(err.T, err)\n",
        "  return err"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL2OqDqTduar"
      },
      "source": [
        "# ordering using PCA Reconstruction Error\n",
        "# as given in paper\n",
        "save_path = '/content/drive/My Drive/'\n",
        "k = 10**4\n",
        "count = 0\n",
        "errs = errlist[0][0]\n",
        "for s in range(len(Pu)):\n",
        "  for i in range(k):\n",
        "    shape = np.array(Pu[s])\n",
        "    points = samplePoints() # sample two points\n",
        "    shape = swap(shape, points)\n",
        "    shapecomp = np.dot(shape.T, comp.T) # transform\n",
        "    shaperecon = np.dot(shapecomp.T, comp) # inverse_transform\n",
        "    err = error(shape, shaperecon)\n",
        "    if err<errs:\n",
        "      Pu[s] = shape\n",
        "      count += 1\n",
        "    else:\n",
        "      continue\n",
        "  print(count, s)\n",
        "  comp = pcaAnalysis(Pu) # calculate new components\n",
        "  #calculate error for next shape\n",
        "  if s<len(Pu-1):\n",
        "    shape = Pu[s+1]\n",
        "    shapecomp = np.dot(shape.T, comp.T)\n",
        "    shaperecon = np.dot(shapecomp.T, comp)\n",
        "    errs = error(shape, shaperecon)\n",
        "  if s%50==0 or s==len(Pu)-1: #calculate errors for each shape and store in errlist\n",
        "    Pcomp = np.dot(Pu, comp.T)\n",
        "    Precon = np.dot(Pcomp, comp)\n",
        "    errors = Pu - Precon\n",
        "    errors = errors*errors\n",
        "    errors = np.sum(errors, axis=1)\n",
        "    errlist.append(errors)\n",
        "    np.save(save_path+'errlist', np.array(errlist))\n",
        "    np.save(save_path+'Pu', Pu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yoaqtdhkMDP"
      },
      "source": [
        "## results after performing swaping for 1000 shapes\n",
        "\n",
        "[Reconstruction Error](https://drive.google.com/file/d/1MGAZNpFGKdh0b-mEuOIDiWvut3WGs_vA/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RR-KR22l62M"
      },
      "source": [
        "def swapAll(Pu, Points):\n",
        "  for i in range(len(points)):\n",
        "    Pu[i] = swap(Pu[i], points[i])\n",
        "  return Pu"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78CFlxNQJxg9"
      },
      "source": [
        "def sampleAll():\n",
        "  points = np.random.randint(0,1000,(5000,2))\n",
        "  return points"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6vKHDuCvmMy"
      },
      "source": [
        "def errAll(P_, comp):\n",
        "  mean = np.mean(P_, axis=0)\n",
        "  Pu_ = P_ - mean\n",
        "  Pcomp = np.dot(Pu_, comp.T)\n",
        "  Precon = np.dot(Pcomp, comp)\n",
        "  errors = Pu_ - Precon\n",
        "  errors = errors*errors\n",
        "  errors = np.sum(errors, axis=1)\n",
        "  return errors"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a-1XxjPv-ve"
      },
      "source": [
        "#### optimized trainig loop\n",
        "save_path = '/content/drive/My Drive/'\n",
        "for iter in range(1000):\n",
        "  for k in range(10*4):\n",
        "    P_ = np.array(P)\n",
        "    points = sampleAll()\n",
        "    P_ = swapAll(P_, points)\n",
        "    # calculate error\n",
        "    err = errAll(P_, comp)\n",
        "    # compare\n",
        "    bin = (err<errors).astype(int)\n",
        "    binP_ = np.array([bin]*3000).T\n",
        "    P = ((1-binP_)*P) + (binP_*P_) # select shapes to be swaped and swap them\n",
        "  Pu = P - np.mean(P, axis=0)\n",
        "  comp = pcaAnalysis(Pu)\n",
        "  errors = errAll(P, comp)\n",
        "  errlist.append(errors)\n",
        "  np.save(save_path+'vec_errlist1', np.array(errlist))\n",
        "  np.save(save_path+'vec_Pu1', P)\n",
        "  print(iter, np.sum(errors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJgUr5HPlNK_"
      },
      "source": [
        "## results after 400 iterations\n",
        "\n",
        "[Reconstruction Error](https://drive.google.com/file/d/1-46E_roxPaFlTNoULcBjBYmD3uCfdoRH/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbSOOr9Jez1T"
      },
      "source": [
        "#### GAN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrT4INXGewqH"
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "from tensorflow.keras.constraints import Constraint\n",
        "from tensorflow.keras import backend"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMroVNaZJY20"
      },
      "source": [
        "class ClipConstraint(Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        " \n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n",
        " \n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-jqSQuWJnyU"
      },
      "source": [
        "const = ClipConstraint(0.01)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-7vQf6CQdAn"
      },
      "source": [
        "latent_dim = 100\n",
        "\n",
        "Generator = keras.Sequential(\n",
        "    [keras.Input(shape=(latent_dim,)),\n",
        "     layers.Dense(100),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.ReLU(),\n",
        "     layers.Dense(100),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.ReLU(),\n",
        "     layers.Dense(100),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.ReLU(),\n",
        "     layers.Dense(100),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.ReLU(),\n",
        "     layers.Dense(100)\n",
        "    ],\n",
        "    name = 'Generator'\n",
        ")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYrx5QMgQiAv"
      },
      "source": [
        "Discriminator = keras.Sequential(\n",
        "    [keras.Input(shape=(latent_dim,)),\n",
        "     layers.Dense(100, kernel_constraint=const),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.LeakyReLU(alpha=0.2),\n",
        "     layers.Dense(100, kernel_constraint=const),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.LeakyReLU(alpha=0.2),\n",
        "     layers.Dense(100, kernel_constraint=const),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.LeakyReLU(alpha=0.2),\n",
        "     layers.Dense(100, kernel_constraint=const),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.LeakyReLU(alpha=0.2),\n",
        "     layers.Dense(1)\n",
        "    ],\n",
        "    name = 'Discriminator'\n",
        ")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvRdyG29Qk2D"
      },
      "source": [
        "def wass_loss_fn(label, coeff):\n",
        "  wgloss = tf.reduce_mean(coeff*label)\n",
        "  return wgloss"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls9QjzinUR0S"
      },
      "source": [
        "def loss_fn(real, fake):\n",
        "  Eloss = tf.reduce_mean(real - fake, axis = 0)\n",
        "  Sloss = tf.reduce_sum(tf.square(Eloss))\n",
        "  return Sloss"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_OS1n-tQor8"
      },
      "source": [
        "## training loop\n",
        "\n",
        "gen_opt = keras.optimizers.Adam()\n",
        "dis_opt = keras.optimizers.Adam()\n",
        "\n",
        "gloss = []\n",
        "dlossr = []\n",
        "dlossf = []\n",
        "\n",
        "@tf.function\n",
        "def train_step(shape_coeff, batch_size):\n",
        "  ## sample noise vectors\n",
        "  noise = tf.random.uniform(shape=[batch_size, 100], minval=-1, maxval=1) ## (batch_size, 100)\n",
        "  ## get generator output\n",
        "  gen_shape_coeff = Generator(noise) ## (batch_size, 100)\n",
        "  label = -tf.ones([batch_size,1])\n",
        "  #comb_shape_coeff = tf.concat([gen_shape_coeff, shape_coeff], axis=0) ## (2*batch_size, 100)\n",
        "  ## generate labels\n",
        "  #labels = tf.concat([tf.ones(gen_shape_coeff.shape[0],1), tf.zeros(shape_coeff.shape[0],1)], axis=0) ## (2*batch_size, 1)\n",
        "  #labels += 0.05*tf.random.uniform(labels.shape) \n",
        "\n",
        "  ## train the discriminator\n",
        "  with tf.GradientTape() as tape:\n",
        "    predf = Discriminator(gen_shape_coeff) ## (2*batch_size, 1)\n",
        "    d_loss_fake = wass_loss_fn(label, predf)\n",
        "  grads = tape.gradient(d_loss_fake, Discriminator.trainable_weights)\n",
        "  dis_opt.apply_gradients(zip(grads, Discriminator.trainable_weights))\n",
        "\n",
        "\n",
        "  label = tf.ones([batch_size,1])\n",
        "  shape_coeff_1 = shape_coeff[:batch_size]\n",
        "  with tf.GradientTape() as tape:\n",
        "    predr = Discriminator(shape_coeff_1)\n",
        "    d_loss_real = wass_loss_fn(label, predr)\n",
        "  grads = tape.gradient(d_loss_real, Discriminator.trainable_weights)\n",
        "  dis_opt.apply_gradients(zip(grads, Discriminator.trainable_weights))\n",
        "\n",
        "  #sample noise for gen training\n",
        "  noise_g = tf.random.uniform(shape=[batch_size,100], minval=-1, maxval=1) ## (batch_size, 100)\n",
        "  #get discriminator output for real data\n",
        "  #dis_output = Discriminator(shape_coeff) ## (x1,x2,x3,x4,pred)\n",
        "  labels = tf.ones([batch_size,1])\n",
        "  shape_coeff_2 = shape_coeff[batch_size:]\n",
        "\n",
        "  ## train the generator\n",
        "  with tf.GradientTape() as tape:\n",
        "    gen_coeff = Generator(noise_g)\n",
        "    pred_g = Discriminator(gen_coeff) ## (x1,x2,x3,x4,pred)\n",
        "    g_loss = loss_fn(shape_coeff_2, gen_coeff) # + wass_loss_fn(labels, pred_g)\n",
        "  grads = tape.gradient(g_loss, Generator.trainable_weights)\n",
        "  gen_opt.apply_gradients(zip(grads, Generator.trainable_weights))\n",
        "  return d_loss_real, d_loss_fake, g_loss, gen_shape_coeff"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTaWEkdIjJ41"
      },
      "source": [
        "Pcomp = Pcomp.astype(\"float32\")"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7xVEYZnQsYc"
      },
      "source": [
        "## set betch_Size and epochs here and run this cell for training\n",
        "\n",
        "batch_size = 2\n",
        "dataset = tf.data.Dataset.from_tensor_slices(Pcomp)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "save_dir = ''\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(\"\\nstart epoch\", epoch)\n",
        "  for step, batch in enumerate(dataset):\n",
        "    # train step\n",
        "    d_loss_real, d_loss_fake, g_loss, gen_shape_coeff = train_step(batch, int(batch_size/2))\n",
        "    gloss.append(g_loss)\n",
        "    dlossr.append(d_loss_real)\n",
        "    dlossf.append(d_loss_fake)\n",
        "\n",
        "    #logging\n",
        "    if step%50==0:\n",
        "      print(\"discriminator loss at step %d:  d_loss_real : %.2f , d_loss_fake : %.2f\" % (step, d_loss_real, d_loss_fake))\n",
        "      print(\"generator loss at step %d: %.2f\" % (step, g_loss))\n",
        "\n",
        "\n",
        "      # save weights and gen_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eryF499LHq2O"
      },
      "source": [
        "plt.plot(np.arange(len(gloss)), gloss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Hu-J52QKXl"
      },
      "source": [
        "plt.plot(np.arange(len(dlossf)), dlossf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-39aDxA4QUbF"
      },
      "source": [
        "plt.plot(np.arange(len(dlossr)), dlossr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SAlqcQaQk05"
      },
      "source": [
        "gen_coeff = Generator(tf.random.uniform([1,100], -1, 1))\n",
        "gen_coeff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cud9cuUXQvX5"
      },
      "source": [
        "gen_coeff = np.array(gen_coeff[0])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjzRpwtbQ59U"
      },
      "source": [
        "gen_shape = np.dot(gen_coeff, comp)\n",
        "gen_shape = gen_shape + Pmean"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTjUmo-zqTXN"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twkZcJKwFZeh"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzzS7ppjFcys"
      },
      "source": [
        "pc0 = gen_shape\n",
        "x = []\n",
        "y = []\n",
        "z = []\n",
        "for i in range(1000):\n",
        "  x.append(pc0[3*i])\n",
        "  y.append(pc0[3*i + 1])\n",
        "  z.append(pc0[3*i + 2])\n",
        "x = np.asarray(x)\n",
        "y = np.asarray(y)\n",
        "z = np.asarray(z)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jehOn5gOGRF9",
        "outputId": "3770cf7c-24d8-4e34-e128-fe3eda3e16ee"
      },
      "source": [
        "ax.scatter(x,z,y)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7ff423da4eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJTw4P2cAtF"
      },
      "source": [
        "fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB8Zr2bV5iZ9"
      },
      "source": [
        "pc = np.load(\"/content/drive/My Drive/vec_Pu1.npy\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CdopdP8qDu_"
      },
      "source": [
        "P = np.asarray(pc)\n",
        "Pmean = np.mean(P, axis=0)\n",
        "Pu = P - Pmean"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psM9YzYE_gfL"
      },
      "source": [
        "comp = pcaAnalysis(Pu)\n",
        "Pcomp = np.dot(Pu, comp.T)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4LYtoC8ADsE"
      },
      "source": [
        "Pcomp = Pcomp.astype('float32')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhJgdj0Klxo6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}